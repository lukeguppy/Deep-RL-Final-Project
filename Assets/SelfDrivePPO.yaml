# behaviors:
#   SelfDrive:
#     trainer_type: sac

#     hyperparameters:
#       learning_rate: 0.0003
#       learning_rate_schedule: constant
#       batch_size: 64
#       buffer_size: 500000
#       buffer_init_steps: 0
#       tau: 0.005
#       steps_per_update: 10.0
#       save_replay_buffer: false
#       init_entcoef: 1.0
#       reward_signal_steps_per_update: 10.0


#     network_settings:
#       normalize: true # Whether to normalise inputs
#       hidden_units: 256 # Typical range: 32 - 512
#       num_layers: 2 # Typical range: 1 - 3
#       vis_encode_type: simple
#       # # memory
#       # memory:
#       #   sequence_length: 64
#       #   memory_size: 256

#     reward_signals:
#       # environment reward (default)
#       extrinsic:
#         gamma: 0.99 # Typical range: 0.8 - 0.995. Discount factor for future rewards coming from the environment
#         strength: 1.0
#       gail:
#         gamma: 0.99
#         strength: 0.01
#         network_settings:
#           normalize: false
#           hidden_units: 128
#           num_layers: 3
#           vis_encode_type: simple
#         learning_rate: 0.0003
#         use_actions: false
#         use_vail: false
#         demo_path: C:\Users\lukew\Project\Car Simulation\demos\Initial Recordin.demo

#     behavioral_cloning:
#       demo_path: C:\Users\lukew\Project\Car Simulation\demos\Initial Recordin.demo
#       steps: 50000
#       strength: 1.0
#       samples_per_update: 0
#     keep_checkpoints: 5 # The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option.
#     max_steps: 100000000 # 0 for infinite
#     time_horizon: 100 # How many steps of experience to collect per-agent before adding it to the experience buffer
#     summary_freq: 30000 # Number of experiences that needs to be collected before generating and displaying training statistics




behaviors:
  SelfDrive:
    trainer_type: ppo

    hyperparameters:
      batch_size: 128 # Number of experiences in each iteration of gradient descent
      # PPO: Number of experiences to collect before updating the policy model
      # SAC: The max size of the experience buffer - on the order of thousands of times longer than your episodes
      buffer_size: 20480 # PPO: 2048 - 409600; SAC: 50000 - 1000000
      learning_rate: 0.001 # Typical range: 1e-5 - 1e-3
      beta: 0.005 # Typical range: 1e-4 - 1e-2. Strength of the entropy regularization, which makes the policy "more random." 
      epsilon: 0.2 # Typical range: 0.1 - 0.3. Influences how rapidly the policy can evolve during training (Corresponds to the acceptable threshold of divergence between the old and new policies during gradient descent)
      lambd: 0.95 # Typical range: 0.9 - 0.95. How much the agent relies on its current value estimate when calculating an updated value estimate
      num_epoch: 5 # Typical range: 3 - 10. Number of passes to make through the experience buffer when performing gradient descent optimization
      learning_rate_schedule: linear 

    network_settings:
      normalize: true # Whether to normalise inputs
      hidden_units: 128 # Typical range: 32 - 512
      num_layers: 3 # Typical range: 1 - 3
      vis_encode_type: simple
      # # memory
      # memory:
      #   sequence_length: 64
      #   memory_size: 256

    reward_signals:
      # environment reward (default)
      extrinsic:
        gamma: 0.99 # Typical range: 0.8 - 0.995. Discount factor for future rewards coming from the environment
        strength: 1.0
      gail:
        gamma: 0.99
        strength: 0.01
        network_settings:
          normalize: false
          hidden_units: 128
          num_layers: 3
          vis_encode_type: simple
        learning_rate: 0.0003
        use_actions: false
        use_vail: false
        demo_path: C:\Users\lukew\Project\Car Simulation\demos\Initial Recordin.demo

    behavioral_cloning:
      demo_path: C:\Users\lukew\Project\Car Simulation\demos\Initial Recordin.demo
      steps: 50000
      strength: 1.0
      samples_per_update: 0
    keep_checkpoints: 5 # The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option.
    max_steps: 100000000 # 0 for infinite
    time_horizon: 100 # How many steps of experience to collect per-agent before adding it to the experience buffer
    summary_freq: 30000 # Number of experiences that needs to be collected before generating and displaying training statistics