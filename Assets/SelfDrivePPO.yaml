behaviors:
  SelfDrive:
    trainer_type: ppo

    hyperparameters:
      batch_size: 250 # Number of experiences in each iteration of gradient descent
      # PPO: Number of experiences to collect before updating the policy model
      # SAC: The max size of the experience buffer - on the order of thousands of times longer than your episodes
      buffer_size: 5000 # PPO: 2048 - 409600; SAC: 50000 - 1000000
      learning_rate: 0.0003 # Typical range: 1e-5 - 1e-3
      beta: 0.003 # Typical range: 1e-4 - 1e-2. Strength of the entropy regularization, which makes the policy "more random." 
      epsilon: 0.25 # Typical range: 0.1 - 0.3. Influences how rapidly the policy can evolve during training (Corresponds to the acceptable threshold of divergence between the old and new policies during gradient descent)
      lambd: 0.925 # Typical range: 0.9 - 0.95. How much the agent relies on its current value estimate when calculating an updated value estimate
      num_epoch: 5 # Typical range: 3 - 10. Number of passes to make through the experience buffer when performing gradient descent optimization
      learning_rate_schedule: linear # linear decays beta linearly, reaching 0 at max_steps, while constant keeps beta constant for the entire training run
      epsilon_schedule: linear

    network_settings:
      normalize: false # Whether to normalise inputs
      hidden_units: 256 # Typical range: 32 - 512
      num_layers: 4 # Typical range: 1 - 3
      vis_encode_type: simple

    reward_signals:
      # environment reward
      extrinsic:
        gamma: 0.99 # Typical range: 0.8 - 0.995. Discount factor for future rewards coming from the environment
        strength: 1.0
      # gail:
      #   gamma: 0.97
      #   strength: 0.1
      #   network_settings:
      #     normalize: false
      #     hidden_units: 64
      #     num_layers: 3
      #     vis_encode_type: simple
      #   learning_rate: 0.0003
      #   use_actions: false
      #   use_vail: false
      #   demo_path: C:\Users\lukew\Project\Car Simulation\demos\InitialRecording.demo
      curiosity:
        strength: 0.1
        gamma: 0.99

    keep_checkpoints: 20 # The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option.
    max_steps: 4500000 # 0 for infinite
    time_horizon: 500 # How many steps of experience to collect per-agent before adding it to the experience buffer
    summary_freq: 20000 # Number of experiences that needs to be collected before generating and displaying training statistics
    threaded: true

