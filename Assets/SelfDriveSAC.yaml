behaviors:
  SelfDrive:
    trainer_type: sac

    hyperparameters:
      learning_rate: 0.0003
      learning_rate_schedule: constant
      batch_size: 256
      buffer_size: 100000
      buffer_init_steps: 0
      tau: 0.005
      steps_per_update: 10.0
      save_replay_buffer: false
      init_entcoef: 1.0
      reward_signal_steps_per_update: 10.0


    network_settings:
      normalize: true # Whether to normalise inputs
      hidden_units: 256 # Typical range: 32 - 512
      num_layers: 3 # Typical range: 1 - 3
      vis_encode_type: simple
      # # memory
      # memory:
      #   sequence_length: 64
      #   memory_size: 256

    reward_signals:
      # environment reward (default)
      extrinsic:
        gamma: 0.99 # Typical range: 0.8 - 0.995. Discount factor for future rewards coming from the environment
        strength: 1.0
      gail:
        gamma: 0.99
        strength: 0.01
        network_settings:
          normalize: false
          hidden_units: 128
          num_layers: 3
          vis_encode_type: simple
        learning_rate: 0.0003
        use_actions: false
        use_vail: false
        demo_path: C:\Users\lukew\Project\Car Simulation\demos\InitialRecording.demo

    # behavioral_cloning:
    #   demo_path: C:\Users\lukew\Project\Car Simulation\demos\InitialRecording.demo
    #   steps: 50000
    #   strength: 1.0
    #   samples_per_update: 0
    keep_checkpoints: 5 # The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option.
    max_steps: 100000000 # 0 for infinite
    time_horizon: 256 # How many steps of experience to collect per-agent before adding it to the experience buffer
    summary_freq: 20000 # Number of experiences that needs to be collected before generating and displaying training statistics
