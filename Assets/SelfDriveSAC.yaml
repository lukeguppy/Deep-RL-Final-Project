behaviors:
  SelfDrive:
    trainer_type: sac

    hyperparameters:
      batch_size: 250
      buffer_size: 50000
      learning_rate: 0.0003
      tau: 0.005
      save_replay_buffer: false
      init_entcoef: 1.0
      reward_signal_steps_per_update: 10.0

    network_settings:
      normalize: false # Whether to normalise inputs
      hidden_units: 256 # Typical range: 32 - 512
      num_layers: 4 # Typical range: 1 - 3
      vis_encode_type: simple

    reward_signals:
      # environment reward 
      extrinsic:
        gamma: 0.99 # Typical range: 0.8 - 0.995. Discount factor for future rewards coming from the environment
        strength: 1.0
      # gail:
      #   gamma: 0.97
      #   strength: 0.1
      #   network_settings:
      #     normalize: false
      #     hidden_units: 64
      #     num_layers: 3
      #     vis_encode_type: simple
      #   learning_rate: 0.0003
      #   use_actions: false
      #   use_vail: false
      #   demo_path: C:\Users\lukew\Project\Car Simulation\demos\InitialRecording.demo
      curiosity:
        strength: 0.1
        gamma: 0.99

    keep_checkpoints: 5 # The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option.
    max_steps: 4000000 # 0 for infinite
    time_horizon: 500 # How many steps of experience to collect per-agent before adding it to the experience buffer
    summary_freq: 20000 # Number of experiences that needs to be collected before generating and displaying training statistics
    threaded: true